# SUMMARY
The authors propose the Knowledge-Oriented LLM Assessment (Cola) to evaluate large language models (LLMs) like GPT-4. Cola uses a four-level cognitive ability taxonomy and evolving data sources to assess LLMs' world knowledge, aiming to provide a comprehensive and fair evaluation.

# IDEAS:
- Traditional benchmarks are insufficient for evaluating advanced LLMs like GPT-4.
- Cola focuses on measuring world knowledge, crucial for LLM performance.
- Cola uses a four-tier cognitive ability taxonomy: memorization, understanding, applying, and creating.
- Evaluations should map relationships between abilities to improve LLMs.
- Training data variations must not affect evaluation fairness.
- Metrics should be user-friendly and insightful.
- Cola uses both known (Wikipedia) and evolving data sources.
- Evolving data sources ensure fair evaluation on unseen content.
- Cola hosts new competition seasons every three months.
- Standardized scores make results comparable across tasks.
- Self-contrast metric evaluates knowledge creation accuracy.
- Larger models tend to remember more knowledge.
- Alignment improves higher-level abilities but may harm memorization.
- Open-source models lag behind commercial counterparts.
- Cola's evaluation system includes standardized scores and self-contrast metrics.
- Self-contrast metric focuses on alignment with presented knowledge.
- Cola's inaugural season evaluated 21 widely used LLMs.
- Cola aims to guide the development of knowledgeable LLMs.
- Cola encourages open participation and contributions from the community.
- Cola provides tools for managing prompts, visualizing data, and accessing results.
- Cola's evolving data source includes web content published in the last 90 days.
- Knowledge memorization tasks probe LLMs' ability to recall facts.
- Knowledge understanding tasks evaluate comprehension of concepts, entities, and events.
- Knowledge applying tasks assess multi-hop reasoning over world knowledge.
- Knowledge creating tasks evaluate the ability to generate coherent and accurate knowledge-based text.
- Standardized scores are adjusted to a range between 0 and 100 for readability.
- Self-contrast metric involves generating two completions to evaluate knowledge creation.
- Cola's platform offers services like competition news, result visualization, and model submission tools.
- Open-source models need stronger collaboration and support for improvement.
- High-level tasks rely heavily on knowledge memory level.
- Evolving data sets enhance generalization capabilities of LLMs.

# INSIGHTS:
- Cola's four-tier taxonomy provides specific feedback on LLM abilities.
- Evolving data sources ensure fair evaluation on newly emerging content.
- Standardized scores make it easier to compare results across different tasks.
- Self-contrast metric enhances evaluation of knowledge creation accuracy.
- Larger models excel in memorization but may struggle with alignment effects.
- Open-source models require more support to compete with commercial ones.
- High-level tasks depend on strong foundational knowledge memory.
- Evolving data sets improve LLMs' generalization capabilities.

# QUOTES:
- "Traditional benchmarks are no longer as useful due to the LLM's expanded capacities."
- "We must ensure that variations in training data do not affect the fairness of evaluations."
- "Our focus is on measuring the world knowledge of LLMs."
- "We have designed a four-tier cognitive ability taxonomy drawing from learning theories such as Bloom's taxonomy."
- "We use both known and evolving data sources."
- "We host a new competition season every three months."
- "Standard scores across tasks make it easier for a wider range of users to understand."
- "Larger models tend to remember more knowledge."
- "Alignment can improve a model's higher-level abilities but might detrimentally affect memorization."
- "Open-source models generally lag behind their commercial counterparts."
- "We invite more LLMs to participate in Cola's evaluations."
- "Cola will guide the development of increasingly knowledgeable LLMs."
- "We believe this nuanced approach to evaluation will prove more effective in assessing and enhancing AI capabilities."
- "We propose evaluations on newly emerging data and maintaining a continuously evolving benchmark."
- "Evaluating knowledge creation involves distinguishing between valid knowledge and false or invented information."
- "Our findings were insightful, showing that larger models tend to remember more knowledge."
- "Alignment unleashes the potential of larger models in higher-level abilities but may harm memorization."
- "Open-source models exhibit overall inferiority compared to commercial models."
- "Cola's evaluation system includes standardized scores and self-contrast metrics."
- "Self-contrast metric focuses on whether the generated completions align with the presented knowledge."

# HABITS:
- Regularly update benchmarks to reflect evolving data sources.
- Use standardized scores for easier comparison across tasks.
- Employ self-contrast metrics to evaluate knowledge creation accuracy.
- Encourage open participation and contributions from the community.
- Provide tools for managing prompts, visualizing data, and accessing results.

# FACTS:
- Traditional benchmarks are insufficient for advanced LLMs like GPT-4.
- Cola uses a four-tier cognitive ability taxonomy: memorization, understanding, applying, and creating.
- Cola hosts new competition seasons every three months using evolving data sources.
- Larger models tend to remember more knowledge but may struggle with alignment effects.
- Open-source models generally lag behind commercial counterparts in performance.

# REFERENCES:
- Wikipedia
- Wikidata 5M
- Hotpot QA
- Two Wiki Multi-hop QA
- Music KQA Pro
- KORC

# ONE-SENTENCE TAKEAWAY
Cola provides a comprehensive evaluation framework for LLMs using evolving data sources and a four-tier cognitive ability taxonomy.

# RECOMMENDATIONS:
- Regularly update benchmarks with newly emerging data sources for fair evaluations.
- Use standardized scores to make results comparable across different tasks.
- Employ self-contrast metrics to enhance evaluation of knowledge creation accuracy.
- Encourage open participation and contributions from the community for continuous improvement.
- Provide tools for managing prompts, visualizing data, and accessing results.