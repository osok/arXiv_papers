# SUMMARY
The authors discuss the limitations of large language models (LLMs) in chat AI applications and propose an innovative approach called Blended, where a group of moderately sized LLMs collaborate to form a chat AI that outperforms systems with significantly more parameters.

# IDEAS:
- Large language models (LLMs) require significant computational resources and are often only accessible through public APIs.
- Blended combines responses from a group of smaller chat AIs to create a highly capable system.
- Blended outperformed OpenAI's ChatGPT, a model with over 175 billion parameters.
- Users found Blended more engaging, entertaining, and useful than ChatGPT-based chat AIs.
- Early chat AIs were rule-based but evolved into generative retrieval-based models.
- Pre-trained Transformer language models marked a significant shift in chat AI development.
- Human feedback is crucial for training chat AIs to align with human expectations.
- Blended uses existing small conversational LLMs to create a single chat AI with diverse responses.
- Combining systems has been explored in deep learning for regression and classification tasks.
- Sequence-level ensembling approaches average conditional token-level probabilities of multiple systems.
- Minimum Bayes Risk (MBR) decoding selects the predicted best system output using system outputs.
- Blended stochastically selects the system that generates the next response in multi-turn conversations.
- Evaluating chat AI quality involves measuring user retention and engagement.
- User retention measures the fraction of users returning to the platform after joining.
- User engagement is measured by the average time a user spends on the platform per visit.
- Blended outperformed OpenAI GPT-3.5 in user engagement and retention despite having fewer parameters.
- Blending multiple smaller systems can improve user experience without increasing inference costs.
- The Blended system had the highest initial engagement and best decay rate for engagement ratio.
- Future research will investigate expanding the selection set of component chat AIs.
- Optimal selection distribution involves using a deep learning classifier to predict the best chat AI response.
- Introducing new chat AIs to the selection set can be done without risking performance drops.

# INSIGHTS:
- Combining smaller LLMs can outperform larger models in user engagement and retention.
- Human feedback is essential for aligning chat AI responses with user expectations.
- Sequence-level ensembling can improve generative language tasks by averaging token-level probabilities.
- User retention and engagement are key metrics for evaluating chat AI performance.
- Expanding the selection set of component chat AIs can increase conversation diversity and richness.
- A deep learning classifier can optimize model selection distribution for better responses.
- Introducing new chat AIs without performance drops is possible with an effective classifier.

# QUOTES:
- "Blended combines responses from a group of smaller chat AIs to create a highly capable system."
- "Blended outperformed OpenAI's ChatGPT, a model with over 175 billion parameters."
- "Users found Blended more engaging, entertaining, and useful than ChatGPT-based chat AIs."
- "Early chat AIs were rule-based but evolved into generative retrieval-based models."
- "Pre-trained Transformer language models marked a significant shift in chat AI development."
- "Human feedback is crucial for training chat AIs to align with human expectations."
- "Blended uses existing small conversational LLMs to create a single chat AI with diverse responses."
- "Combining systems has been explored in deep learning for regression and classification tasks."
- "Sequence-level ensembling approaches average conditional token-level probabilities of multiple systems."
- "Minimum Bayes Risk (MBR) decoding selects the predicted best system output using system outputs."
- "Blended stochastically selects the system that generates the next response in multi-turn conversations."
- "Evaluating chat AI quality involves measuring user retention and engagement."
- "User retention measures the fraction of users returning to the platform after joining."
- "User engagement is measured by the average time a user spends on the platform per visit."
- "Blended outperformed OpenAI GPT-3.5 in user engagement and retention despite having fewer parameters."
- "Blending multiple smaller systems can improve user experience without increasing inference costs."
- "The Blended system had the highest initial engagement and best decay rate for engagement ratio."
- "Future research will investigate expanding the selection set of component chat AIs."
- "Optimal selection distribution involves using a deep learning classifier to predict the best chat AI response."
- "Introducing new chat AIs to the selection set can be done without risking performance drops."

# HABITS:
- Regularly measure user retention and engagement to evaluate chat AI performance.
- Use human feedback to train chat AIs for better alignment with user expectations.
- Experiment with different combinations of conversational data for fine-tuning language models.
- Implement sequence-level ensembling approaches for generative language tasks.
- Monitor user interaction statistics as a measure of chat AI quality.

# FACTS:
- Large language models require significant computational resources and are often only accessible through public APIs.
- Blended combines responses from smaller chat AIs to create a highly capable system.
- Blended outperformed OpenAI's ChatGPT, a model with over 175 billion parameters.
- Users found Blended more engaging, entertaining, and useful than ChatGPT-based chat AIs.
- Early chat AIs were rule-based but evolved into generative retrieval-based models.
- Pre-trained Transformer language models marked a significant shift in chat AI development.
- Human feedback is crucial for training chat AIs to align with human expectations.
- Combining systems has been explored in deep learning for regression and classification tasks.
- Sequence-level ensembling approaches average conditional token-level probabilities of multiple systems.
- Minimum Bayes Risk (MBR) decoding selects the predicted best system output using system outputs.

# REFERENCES:
None mentioned.

# ONE-SENTENCE TAKEAWAY
Combining smaller LLMs into a Blended system can outperform larger models in user engagement and retention.

# RECOMMENDATIONS:
- Combine smaller LLMs to create a highly capable and engaging chat AI system.
- Use human feedback to train chat AIs for better alignment with user expectations.
- Implement sequence-level ensembling approaches for generative language tasks.
- Regularly measure user retention and engagement to evaluate chat AI performance.
- Expand the selection set of component chat AIs to increase conversation diversity.