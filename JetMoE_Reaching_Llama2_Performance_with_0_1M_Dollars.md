# SUMMARY
The text discusses the challenges of large language models (LLMs) and introduces JetMo 8B, an innovative mixture of experts (MoE) architecture that reduces computational costs while maintaining high performance.

# IDEAS:
- High resource demands hinder the development of powerful and accessible AI.
- Modern LLMs surpass humans in some tasks but remain inefficient and rigid.
- Dense models use all parameters during both training and inference.
- Mixture of Experts (MoE) architecture scales parameters while keeping computational costs reasonable.
- Recent MoE advancements applied to Transformers have led to successful scaling attempts.
- Models like Deep Seek Mo, MixL 8X 7B, Grock 1, and DBRX show remarkable performance.
- These models are not fully open-sourced due to proprietary training methods.
- Open-source community efforts like Open Moe perform comparably to weak dense models.
- JetMo 8B extends sparse activation to both attention and feed-forward layers.
- JetMo 8B outperforms models like Llama 2 to 7B and Llama 2 to 13B Chat.
- JetMo 8B is trained with a limited budget and open-source datasets.
- Sparse activation in both components reduces computational costs while maintaining performance.
- JetMo's MoE layer consists of modules and a router selecting top experts based on probability distribution.
- Each feed-forward expert in JetMo is a standard two-layer MLP with specific input and output projections.
- Mixture of Attention Heads (MOA) enhances SMoEs with attention mechanisms.
- MOA allows multiple heads per expert and incorporates relative positioning into attention calculation.
- Load balancing techniques prevent one module from being overused during pre-training.
- JetMo training uses frequency-based auxiliary laws and router Z loss for stability.
- The refined web dataset contains 5 trillion tokens refined from Common Crawl using the MDR pipeline.
- Star Coder training data sourced from GitHub across 86 programming languages.
- Dolma Corpus comprises 3 trillion tokens from various sources including web pages, code, social media, academic papers, public domain books, and encyclopedic content.
- The Pile dataset is an 825 GB English text corpus comprising diverse publicly available datasets.
- Ultra Textbooks dataset includes high-quality synthetic and human-written textbooks from various sources.
- Ultra Chat 200k subset filters out dialogues generated by ChatGPT for grammatical correctness.
- Template GSM dataset contains grade school math problems with solutions in code and natural language.
- Magic Coder Evol 110k and Magic Coder O 75k datasets generated using the O Instruct approach.
- Evel Code Alpaca is an open-source implementation tailored for code instructions.
- Megatron framework integrates Mega Block for support in MoE training.
- Pipeline parallelism is preferred over expert parallelism for efficiency in sparse MoE models.
- Distilled Supervised Fine-Tuning (DSFT) trains a student model using data generated by a teacher model like GPT-4 or Claude.
- Distilled Direct Preference Optimization (DDPO) refines DSFT models by incorporating preferences from an aligned teacher model.
- DDPO optimizes a reward function to align student model outputs with desired outcomes.
- KL-constrained optimization aims to derive the optimal policy maximizing expected rewards while minimizing divergence from a baseline policy.
- Offline DPO optimizes language model policies using static preference data offering stable learning compared to RLHF.

# INSIGHTS:
- High resource demands limit the accessibility of powerful AI models.
- Sparse activation in both attention and feed-forward layers reduces computational costs significantly.
- Open-source efforts are crucial for advancing accessible AI research.
- Mixture of Experts (MoE) architecture offers a scalable solution for LLMs.
- Load balancing techniques ensure efficient utilization of all model modules during training.
- Diverse datasets enhance the training quality of language models.
- Pipeline parallelism improves efficiency in sparse MoE models compared to expert parallelism.
- Distilled Supervised Fine-Tuning (DSFT) leverages teacher models to train student models effectively.
- Distilled Direct Preference Optimization (DDPO) aligns student model outputs with desired outcomes using reward functions.

# QUOTES:
- "High resource demands hinder the development of powerful and accessible AI."
- "Modern LLMs surpass humans in some tasks but remain inefficient and rigid."
- "Dense models use all parameters during both training and inference."
- "Mixture of Experts (MoE) architecture scales parameters while keeping computational costs reasonable."
- "Recent MoE advancements applied to Transformers have led to successful scaling attempts."
- "Models like Deep Seek Mo, MixL 8X 7B, Grock 1, and DBRX show remarkable performance."
- "These models are not fully open-sourced due to proprietary training methods."
- "Open-source community efforts like Open Moe perform comparably to weak dense models."
- "JetMo 8B extends sparse activation to both attention and feed-forward layers."
- "JetMo 8B outperforms models like Llama 2 to 7B and Llama 2 to 13B Chat."
- "JetMo 8B is trained with a limited budget and open-source datasets."
- "Sparse activation in both components reduces computational costs while maintaining performance."
- "JetMo's MoE layer consists of modules and a router selecting top experts based on probability distribution."
- "Each feed-forward expert in JetMo is a standard two-layer MLP with specific input and output projections."
- "Mixture of Attention Heads (MOA) enhances SMoEs with attention mechanisms."
- "MOA allows multiple heads per expert and incorporates relative positioning into attention calculation."
- "Load balancing techniques prevent one module from being overused during pre-training."
- "JetMo training uses frequency-based auxiliary laws and router Z loss for stability."
- "The refined web dataset contains 5 trillion tokens refined from Common Crawl using the MDR pipeline."
  
# HABITS:
- Utilizing sparse activation in both attention and feed-forward layers for efficiency.
- Training with a limited budget using open-source datasets for accessibility.
- Implementing load balancing techniques during pre-training for efficient module utilization.
  
# FACTS:
- High resource demands limit the accessibility of powerful AI models.
- Dense models use all parameters during both training and inference.
  
# REFERENCES:
None provided.

# ONE-SENTENCE TAKEAWAY
JetMo 8B's innovative sparse activation approach significantly reduces computational costs while maintaining high performance, advancing accessible AI research.

# RECOMMENDATIONS:
None provided.